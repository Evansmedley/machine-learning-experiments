{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ZYySLEBW-4wj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuKtqf8088A0"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "# Type annotations\n",
        "from typing import Tuple\n",
        "\n",
        "# General\n",
        "import os\n",
        "import zipfile\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoProcessor, ASTModel\n",
        "\n",
        "# Image preprocessing\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Model\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim   // Optimizer\n",
        "from tqdm import tqdm         // Progress bar\n",
        "import torchsummary           // Model summary\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import  accuracy_score\n",
        "from sklearn.metrics import  precision_score\n",
        "from sklearn.metrics import  recall_score\n",
        "from sklearn.metrics import  f1_score\n",
        "from sklearn.metrics import  classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import  roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up file structure"
      ],
      "metadata": {
        "id": "jZx7nc6Z-9Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ttgWL5dS9mk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip\n",
        "zip_file_paths = ['/content/drive/My Drive/GTZAN Genre Collection.zip',\n",
        "                  '/content/drive/My Drive/GTZAN Genre Collection Spectrograms.zip']\n",
        "\n",
        "dataset_dirs = ['/content/drive/My Drive/GTZAN Genre Collection',\n",
        "                '/content/drive/My Drive/GTZAN Genre Collection Spectrograms']\n",
        "\n",
        "for zip_file_path, dataset_dir in tqdm(zip(zip_file_paths, dataset_dirs)):\n",
        "    if os.path.exists(zip_file_path):\n",
        "        print(f\"Extracting {zip_file_path} to {dataset_dir}\")\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(dataset_dir)\n",
        "    else:\n",
        "        print(f\"Zip file {zip_file_path} does not exist.\")\n"
      ],
      "metadata": {
        "id": "l6xLoRZG9rbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate dataset size and experiment with data visualization"
      ],
      "metadata": {
        "id": "QrkLKhVY_AUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and display sample distribution count\n",
        "dataset_dirs = ['/content/drive/My Drive/GTZAN Genre Collection',\n",
        "                '/content/drive/My Drive/GTZAN Genre Collection Spectrograms']\n",
        "def load_class_info(dataset_path: str):\n",
        "  df_contents = {'file': [], 'class': []}\n",
        "  for c in os.listdir(dataset_path):\n",
        "    for image_name in os.listdir(f'{dataset_path}/{c}'):\n",
        "      df_contents['file'].append(image_name)\n",
        "      df_contents['class'].append(c)\n",
        "  return pd.DataFrame(df_contents)\n",
        "\n",
        "\n",
        "def display_df_info(df_name: str, df) -> None:\n",
        "  num_classes = df['class'].nunique()\n",
        "  print(f\"'{df_name}' has a total of {len(df.axes[0])} samples distributed over {num_classes} classes.\")\n",
        "\n",
        "\n",
        "audio_df = load_class_info(dataset_dirs[0] + \"/genres_original\")\n",
        "spectrogram_df = load_class_info(dataset_dirs[1] + \"/images_original\")\n",
        "\n",
        "display_df_info('Audio Dataset', audio_df)\n",
        "display_df_info('Spectrogram Dataset', spectrogram_df)\n",
        "\n",
        "print(\"Note that for some reason the spectrogram dataset is missing one sample (jazz00054.png).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "cYJ_ZxnJ9yAJ",
        "outputId": "cc400ad8-1bf0-48f2-f03e-543dd8c1b291"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ee304247a2a>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0maudio_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_class_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/genres_original\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mspectrogram_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_class_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/images_original\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6ee304247a2a>\u001b[0m in \u001b[0;36mload_class_info\u001b[0;34m(dataset_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_class_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdf_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{dataset_path}/{c}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mdf_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of dataset\n",
        "def plot_distribution(title: str, df):\n",
        "  plt.title(title)\n",
        "  dictionary = df['class'].value_counts().to_dict()\n",
        "  plt.bar(range(len(dictionary)), list(dictionary.values()))\n",
        "  plt.xticks(range(len(dictionary)), list(dictionary.keys()))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_distribution('Audio Dataset', audio_df)\n",
        "\n",
        "# Not using this spectrogram but here it is just to see\n",
        "plot_distribution('Spectrogram Dataset', spectrogram_df)"
      ],
      "metadata": {
        "id": "qcTdCi0N94od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hooray, balanced dataset (mostly)!"
      ],
      "metadata": {
        "id": "d30aX4a698eV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load one audio file\n",
        "y, sr = librosa.load(dataset_dirs[0] + \"/genres_original/rock/rock.00000.wav\")\n",
        "\n",
        "# Show the waveform\n",
        "librosa.display.waveshow(y, sr=sr)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "'''\n",
        "Generate mel spectrogram on the log scale, this is the spectrogram that will\n",
        "be used instead of the given spectrogram in an attempt to improve model\n",
        "performance.\n",
        "'''\n",
        "mel_spectrogram_decibels = librosa.power_to_db(\n",
        "    librosa.feature.melspectrogram(y=y, sr=sr),\n",
        "    ref=np.max\n",
        ")\n",
        "librosa.display.specshow(mel_spectrogram_decibels)\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "RRdjZ36L-Cs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "3fvX5C0h-3fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the Mel Frequency Spectrogram from an audio file. It's supposed to help with timbre visualization and is used widely ([hugging face link](https://huggingface.co/learn/audio-course/en/chapter1/audio_data))."
      ],
      "metadata": {
        "id": "UcHKHnt-_YY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "\n",
        "# Loop through all audio files and create spectrogram pngs for each, save to drive\n",
        "def preprocess_audio(y, sr: int) -> None:\n",
        "  new_sr = 22050\n",
        "  y = librosa.resample(y, orig_sr=sr, target_sr=new_sr)\n",
        "  y = librosa.to_mono(y)\n",
        "  y = librosa.util.fix_length(y, size=new_sr*29)\n",
        "  return y, new_sr\n",
        "\n",
        "\n",
        "def generate_mel_spectrogram(y, sr: int, save_path: str) -> None:\n",
        "  mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "  decibel_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  librosa.display.specshow(decibel_mel_spectrogram, sr=sr)\n",
        "  plt.savefig(save_path, bbox_inches='tight')\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "for directory, _, files in os.walk(dataset_dirs[0] + \"/genres_original\"):\n",
        "  for file in tqdm(files, desc=f\"Processing all files in '{directory}'\"):\n",
        "    if 'jazz' in file and '00054' in file:\n",
        "      # Skip\n",
        "      a = 0\n",
        "    else:\n",
        "      y, sr = librosa.load(os.path.join(directory, file))\n",
        "      y, sr = preprocess_audio(y, sr)\n",
        "      generate_mel_spectrogram(y, sr, f'{dataset_dirs[1]}/images_original/{file.split(\".\")[0]}/{file.split(\".wav\")[0].replace(\".\", \"\")}.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "hBCwBpeN-z_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test (no validation for now)\n",
        "\n",
        "# Spectrograms have been generated above, import with ImageFolder\n",
        "IMAGE_SIZE = (240, 180)\n",
        "\n",
        "spectrogram_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.Grayscale(1),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "mel_spec_dataset = ImageFolder(dataset_dirs[1] + \"/images_original\", spectrogram_transforms)\n",
        "\n",
        "# Perform split\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def split_dataset(ds_name: str, ds: Dataset) -> Tuple[Dataset, Dataset]:\n",
        "  ds_train, ds_test = random_split(ds, [0.8, 0.2])\n",
        "  print(f'{ds_name} -> train size: {len(ds_train)}, test_size: {len(ds_test)}')\n",
        "  return ds_train, ds_test\n",
        "\n",
        "\n",
        "def get_loaders(train: Dataset, test: Dataset, batch_size: int) -> Tuple[DataLoader, DataLoader]:\n",
        "  train_loader = DataLoader(train, batch_size=batch_size, num_workers=1, shuffle=True)\n",
        "  test_loader = DataLoader(test, batch_size=batch_size, num_workers=1, shuffle=True)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "spectrogram_train, spectrogram_test = split_dataset(\"Spectrogram Dataset\", mel_spec_dataset)\n",
        "spectrogram_train_loader, spectrogram_test_loader = get_loaders(spectrogram_train, spectrogram_test, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "gbdN_-2W_2YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Covolutional Neural Network\n",
        "- 3 convolutional layers with batch norm and a ReLU activation function\n",
        "- 3 max pooling layers\n",
        "- 1 fully connected layer with dropout to avoid overfitting\n",
        "- First convolutional layer has a large kernel size in order to consider context of a group of pixels and identify patterns (Idea borrowed from [AlexNet](https://pytorch.org/vision/main/models/generated/torchvision.models.alexnet.html))"
      ],
      "metadata": {
        "id": "6Q6yX7J8AF7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    # Convolutional layers\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, kernel_size=8, stride=2, padding=1),\n",
        "        # Dimensions are 158, 118\n",
        "        nn.BatchNorm2d(32, momentum=0.4),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64, momentum=0.4),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(128, momentum=0.4),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "    )\n",
        "\n",
        "    # Fully connected layers\n",
        "    self.f_conn_layers = nn.Sequential(\n",
        "        nn.Dropout(p=0.6),\n",
        "        nn.Linear(in_features=128 * ((IMAGE_SIZE[0] - 6)//2 + 1) * ((IMAGE_SIZE[1] - 6)//2 + 1), out_features=10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_layers(x)\n",
        "    x = x.view(-1, 128 * ((IMAGE_SIZE[0] - 6)//2 + 1) * ((IMAGE_SIZE[1] - 6)//2 + 1))\n",
        "    x = self.f_conn_layers(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "apiB13IUBLHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-5\n",
        "WEIGHT_DECAY = 0.03\n",
        "\n",
        "# Initialize model\n",
        "cnn = CNN()\n",
        "print(cnn)\n",
        "\n",
        "# Use cuda cores\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_cuda = cnn.to(device)\n",
        "\n",
        "# Setup optimizer and loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(cnn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
      ],
      "metadata": {
        "id": "DLQQOdjjBPmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "tv8RVgsQixbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "EPOCHS = 14\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cnn_cuda = cnn.to(device)\n",
        "\n",
        "# Train\n",
        "for epoch in range(EPOCHS):\n",
        "  batch_losses = []\n",
        "  print(f'Epoch: {epoch}')\n",
        "  cnn_cuda.train()\n",
        "  for images, labels in tqdm(spectrogram_train_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = cnn_cuda(images)\n",
        "    loss = loss_fn(logits, labels)\n",
        "    batch_losses.append(loss)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Epoch loss: {sum(batch_losses)/len(batch_losses)}')"
      ],
      "metadata": {
        "id": "8lqUREtxizvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "MpMLAn1Fi7At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test using accuracy metric\n",
        "cnn_cuda.eval()\n",
        "num_test_samples = len(spectrogram_test_loader.dataset)\n",
        "outputs = torch.empty(num_test_samples, dtype=torch.long, device='cuda')\n",
        "targets = torch.empty(num_test_samples, dtype=torch.long, device='cuda')\n",
        "\n",
        "for i, (images, labels) in enumerate(tqdm(spectrogram_test_loader)):\n",
        "  images, labels = images.cuda(), labels.cuda()\n",
        "  with torch.no_grad():\n",
        "    logits = cnn_cuda(images)\n",
        "\n",
        "  outputs[i * BATCH_SIZE:i * BATCH_SIZE + BATCH_SIZE] = torch.argmax(logits, dim=1)\n",
        "  targets[i * BATCH_SIZE:i * BATCH_SIZE + BATCH_SIZE] = labels\n",
        "\n",
        "\n",
        "acc = accuracy_score(outputs.cpu(), targets.cpu())\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "L4aV5iWIi8Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "def cm():\n",
        "  cm = confusion_matrix(outputs.cpu(), targets.cpu())\n",
        "  classes = spectrogram_df['class'].unique()\n",
        "  cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "  sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "\n",
        "cm()"
      ],
      "metadata": {
        "id": "dyXaF5UQjDE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying the same thing using resnet (transfer learning) to compare accuracy"
      ],
      "metadata": {
        "id": "7hLh3QI_jJ8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50"
      ],
      "metadata": {
        "id": "8E3TW2ZnjWrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resnet requires three input channels so must redefine dataset transformations to get RGB images again (can't use grey scale ones)."
      ],
      "metadata": {
        "id": "U-60CaUMjh1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load data as RGB\n",
        "\n",
        "# Same transforms as before but not greyscale!\n",
        "spectrogram_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset with new transforms\n",
        "rgb_dataset = ImageFolder(dataset_dirs[1] + \"/images_original\", spectrogram_transforms)\n",
        "\n",
        "# Initialize loaders (using same functions as above)\n",
        "transfer_train, transfer_test = split_dataset(\"Spectrogram Dataset RGB\", rgb_dataset)\n",
        "transfer_train_loader, transfer_test_loader = get_loaders(transfer_train, transfer_test, BATCHs_SIZE)\n"
      ],
      "metadata": {
        "id": "bV3Dfp8KjfpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize model with pretrained weights"
      ],
      "metadata": {
        "id": "EfYGppOckCWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ResNet50\n",
        "pretrained_model = resnet50(pretrained=True)\n",
        "\n",
        "# Freeze gradient on pre-trained weights\n",
        "for param in pretrained_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Configure output layer (to train)\n",
        "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
        "\n",
        "# Loss function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(pretrained_model.parameters(), lr=0.001, weight_decay=0.9)\n"
      ],
      "metadata": {
        "id": "qG7V8zA2j3R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "5XX9fzdekA9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pretrained_model.to(device)\n",
        "\n",
        "# Train\n",
        "for epoch in range(10):\n",
        "  batch_losses = []\n",
        "  print(f'Epoch: {epoch}')\n",
        "  pretrained_model.train()\n",
        "  for images, labels in tqdm(transfer_train_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = pretrained_model(images)\n",
        "    loss = loss_fn(logits, labels)\n",
        "    batch_losses.append(loss)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Epoch loss: {sum(batch_losses)/len(batch_losses)}')"
      ],
      "metadata": {
        "id": "5stlggi7j_3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "MSjg8UorkHKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model accuracy\n",
        "pretrained_model.eval()\n",
        "num_test_samples = len(transfer_test_loader.dataset)\n",
        "outputs = torch.empty(num_test_samples, dtype=torch.long, device='cuda')\n",
        "targets = torch.empty(num_test_samples, dtype=torch.long, device='cuda')\n",
        "\n",
        "for i, (images, labels) in enumerate(tqdm(transfer_test_loader)):\n",
        "  images, labels = images.cuda(), labels.cuda()\n",
        "  with torch.no_grad():\n",
        "    logits = pretrained_model(images)\n",
        "\n",
        "  outputs[i * BATCH_SIZE:i * BATCH_SIZE + BATCH_SIZE] = torch.argmax(logits, dim=1)\n",
        "  targets[i * BATCH_SIZE:i * BATCH_SIZE + BATCH_SIZE] = labels\n",
        "\n",
        "\n",
        "acc = accuracy_score(outputs.cpu(), targets.cpu())\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "ZLd1EG19kIpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "pretrained_cm()"
      ],
      "metadata": {
        "id": "EUqcGzePkJdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}